{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 循环神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 序列模型\n",
    "\n",
    "序列模型的现象：\n",
    "1. 锚定效应：基于其他人的意见做出评价\n",
    "2. 享乐适应：人们迅速适应一种更好或更坏的情况作为新的常态\n",
    "3. 季节性"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 统计工具\n",
    "\n",
    "1. 自回归模型\n",
    "2. 马尔可夫模型\n",
    "3. 因果关系\n",
    "\n",
    "预测一个序列未来值$x_t$：\n",
    "\n",
    "$$\n",
    "x_t~P(x_t|x_{t-1},...,x_1)\n",
    "$$\n",
    "\n",
    "### 自回归模型\n",
    "\n",
    "1. 自回归模型：在现实情况下$x_{t-1},...,x_1$是不必要的，我们假设在$t>\\tau$时，x_t~P(x_t|x_{t-1},...,x_{t-\\tau})\n",
    "2. 隐变量自回归模型：保留一些对过去观测的总结$h_t$，这样就产生了基于$x_t^{‘}=P(x_t|h_t)$\n",
    "\n",
    "### 马尔可夫模型\n",
    "\n",
    "只要上述自回归模型是近似精确的，我们就说序列满足马尔可夫条件。这样的模型可以使用动态规划沿着马尔可夫链精确的计算结果。\n",
    "\n",
    "利用这样的事实，我们只需要考虑过去观察中的一个非常短的历史。\n",
    "\n",
    "### 因果关系\n",
    "\n",
    "解释$P(x_{t+1}|x_{t})$应当比解释$P(x_{t}|x_{t+1})$更容易"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 文本预处理\n",
    "\n",
    "1. 将文本作为字符串加载到内存中\n",
    "2. 将字符串拆分为词元（一般使用$jieba$等分词器\n",
    "3. 建立一个词表，将拆分的词元映射到数字索引\n",
    "4. 将文本转换为数字索引序列，以方便模型操作\n",
    "\n",
    "未知词元：$<unk>$\n",
    "填充词元：$<pad>$\n",
    "序列开始词元：$<bos>$\n",
    "序列结束词元：$<eos>$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 语言模型\n",
    "\n",
    "文本序列可以看做词元的序列，$[x_1,x_2,…,x_T]$，其中$x_t$可以看做文本序列在时间步$t$处的观测指标。\n",
    "\n",
    "### 学习语言模型\n",
    "\n",
    "基本概率规则：\n",
    "$$\n",
    "P(x_1,x_2,…,x_T) = \\Pi_{t=1}^{T} P(x_t|x_1,…,x_{t-1})\n",
    "$$\n",
    "\n",
    "拉普拉斯平滑：（解决数据集过小或单词罕见的问题）\n",
    "$$\n",
    "P(x)=\\frac{n(x)+\\epsilon/m}{n+\\epsilon}\n",
    "$$\n",
    "其中，$n$表示训练集中单词总数，$m$表示唯一单词的数量\n",
    "\n",
    "### 马尔可夫模型与$n$元语法\n",
    "\n",
    "如果$P(x_{t+1}|x_1,…,x_{t}) = P(x_{t+1}|x_{t})$，则序列上的分布满足一阶马尔可夫性质。阶数越高，对应的依赖关系链就越长。\n",
    "\n",
    "### 自然语言统计\n",
    "\n",
    "齐普夫定律：第$i$个常用的单词的频率$n_i$满足$n_i\\propto \\frac{1}{i^\\alpha}$\n",
    "\n",
    "上述公式等价于：\n",
    "$$\n",
    "logn_i=-\\alpha logi+c\n",
    "$$\n",
    "其中，$\\alpha$是描述分布的指数，$c$是常数。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 循环神经网络\n",
    "\n",
    "### 无隐状态的神经网络\n",
    "$$\n",
    "H=\\phi(XW_{xh}+b_h)\n",
    "$$\n",
    "\n",
    "### 有隐状态的循环神经网络\n",
    "$$\n",
    "H_t=\\phi(X_tW_{xh}+H_{t-1}W_{hh}+b_h)\n",
    "$$\n",
    "\n",
    "其中$X$表示自变量，$W$表示隐藏层的权重参数，$b$代表偏置\n",
    "\n",
    "### 困惑度\n",
    "\n",
    "使用$n$个词元的交叉熵损失函数来衡量：\n",
    "$$\n",
    "\\frac{1}{n}\\Sigma_{t=1}^n-logP(x_t|x_1,…,x_{t-1})\n",
    "$$\n",
    "\n",
    "困惑度即为：\n",
    "$$\n",
    "e^{-\\frac{1}{n}\\Sigma_{t=1}^nlogP(x_t|x_1,…,x_{t-1})}\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}